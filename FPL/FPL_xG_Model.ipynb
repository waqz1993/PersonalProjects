{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb6d1b3c",
   "metadata": {},
   "source": [
    "# Contents\n",
    "1. Introduction <br>\n",
    "    1.1. Import Libraries and Data<br>\n",
    "    \n",
    "    \n",
    "2. Pipelines <br> \n",
    "    2.1. Cleaning Data Pipeline - Classes <br>\n",
    "    2.2. Cleaning Data Pipeline - Application <br>\n",
    "    2.3. Pre-processing Data and Model Pipeline <br>\n",
    "    2.4. Model Prediction <br> \n",
    "    \n",
    "    \n",
    "3. Goal Correlation (xG vs Model) <br>\n",
    "    3.1. Goal Correlation with xG <br>\n",
    "    3.2. Goal Correlation with Model <br>\n",
    "    3.3. Summary <br> \n",
    " \n",
    " \n",
    "4. XGB Regression Hyperparameter Optimization <br>\n",
    "    4.1. Before Optimization <br>\n",
    "    4.2. Optimization <br>\n",
    "    4.3. After Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3583d9",
   "metadata": {},
   "source": [
    "# 1. Introduction \n",
    "\n",
    "In this kernel we create an XGBoost Regressor model from various independent factors drawn from our main kernel to better predict goals scored. \n",
    "\n",
    "We will show our model has a higher correlation with goals than xG alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbd80f3",
   "metadata": {},
   "source": [
    "## 1.1. Import Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1446,
   "id": "3377eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, TimeSeriesSplit\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1328,
   "id": "b90b2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dfFinal.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305e3d0",
   "metadata": {},
   "source": [
    "# 2. Pipelines\n",
    "\n",
    "## 2.1. Cleaning Data Pipeline - Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1329,
   "id": "6961b0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort by date - useful when splitting df (time-series). \n",
    "class sort_date(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.sort_values(by=['date'])\n",
    "\n",
    "#Drop columns. \n",
    "class drop_col(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.drop(['player', 'Unnamed: 0', '#', 'game_id', 'Cmp%', 'date', 'Opposition', 'team', 'Nation'], axis=1)\n",
    "\n",
    "#Age column clean up. \n",
    "class age_dtype(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X['Age']=X['Age'].replace(np.nan, 0)\n",
    "        X['Age']=(X['Age'].astype(str).str[:2]).astype(float)\n",
    "        X['Age']=X['Age'].replace(0, np.nan)\n",
    "        return X \n",
    "\n",
    "#One hot encode position col separately. Not ideal since novel categories can come up in new data but had to because multiple\n",
    "#categ's in col. Change this after model trained. \n",
    "class position_ohe(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X=pd.concat([X, X['Pos'].str.get_dummies(',')], axis=1)\n",
    "        X.drop(['Pos'], axis=1, inplace=True)\n",
    "        return X \n",
    "\n",
    "#Remove NaN's. \n",
    "class remove_nan(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X.dropna(inplace=True)\n",
    "        return X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bbbed7",
   "metadata": {},
   "source": [
    "## 2.2. Cleaning Data Pipeline - Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1411,
   "id": "fc659c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pipe = Pipeline([\n",
    "    (\"sort_date\", sort_date()),\n",
    "    (\"drop_columns\", drop_col()),\n",
    "    (\"age_datatype\", age_dtype()),\n",
    "    (\"encoding_position_column\", position_ohe()),\n",
    "    (\"remove_nan\", remove_nan())\n",
    "])\n",
    "\n",
    "cleaned_df = clean_pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213cdeb",
   "metadata": {},
   "source": [
    "## 2.3. Pre-Processing Data and Model Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f07b94",
   "metadata": {},
   "source": [
    "### Pre-Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1440,
   "id": "8faead4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = Pipeline(steps=[\n",
    "        (\"standard_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "robust_scaler = Pipeline(steps=[\n",
    "        (\"robust_scaler\", RobustScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"stand_scaler\", standard_scaler, [\"Touches\", \"Age\"]),\n",
    "        (\"robust_scaler\", robust_scaler, [\"Att\", \"Cmp\", \"Mins\"]),\n",
    "        (\"cat\", categorical_transformer, [\"Location\"])\n",
    "        ], remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "lin = xgb.XGBRegressor(\n",
    "        min_child_weight= 1,\n",
    "        max_depth= 4,\n",
    "        learning_rate= 0.15,\n",
    "        gamma=3,\n",
    "        colsample_bytree=0.75\n",
    ")\n",
    "\n",
    "xgb_model = Pipeline(steps= [\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"XGB Model\", lin)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1ac94f",
   "metadata": {},
   "source": [
    "## 2.4. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1441,
   "id": "f0a34b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('pre',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('stand_scaler',\n",
       "                                                  Pipeline(steps=[('standard_scaler',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Touches', 'Age']),\n",
       "                                                 ('robust_scaler',\n",
       "                                                  Pipeline(steps=[('robust_scaler',\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  ['Att', 'Cmp', 'Mins']),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('encoder',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Location'])])...\n",
       "                              feature_types=None, gamma=3, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=0.15,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=4, max_leaves=None, min_child_weight=1,\n",
       "                              missing=nan, monotone_constraints=None,\n",
       "                              n_estimators=100, n_jobs=None,\n",
       "                              num_parallel_tree=None, predictor=None,\n",
       "                              random_state=None, ...))])"
      ]
     },
     "execution_count": 1441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = np.split(cleaned_df, [int(.8 *len(cleaned_df))]) #change when needed \n",
    "\n",
    "X_train = train_df.loc[:, train_df.columns != 'Gls']\n",
    "X_test = test_df.loc[:, test_df.columns != 'Gls']\n",
    "\n",
    "y_train = train_df.loc[:, train_df.columns == 'Gls']\n",
    "y_test = test_df.loc[:, test_df.columns == 'Gls']\n",
    "\n",
    "xgb_model_fit=xgb_model.fit(X_train, y_train)\n",
    "xgb_model_prediction=xgb_model_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7722126",
   "metadata": {},
   "source": [
    "# 3. Goal Correlation (xG vs Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a2b3d1",
   "metadata": {},
   "source": [
    "## 3.1. Goal Correlation with xG "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2867955",
   "metadata": {},
   "source": [
    "###  Time Series Cross Validation on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1442,
   "id": "bed8bacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2606475401563377 +/- 0.009597410120866906 - Training\n",
      "R2: 0.3679386630057572 +/- 0.0073866866216153 - Training\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = np.split(cleaned_df, [int(.8 *len(cleaned_df))])\n",
    "\n",
    "X_train = train_df.loc[:, train_df.columns == 'xG']\n",
    "y_train = train_df.loc[:, train_df.columns == 'Gls']\n",
    "\n",
    "X_test = test_df.loc[:, test_df.columns == 'xG']\n",
    "y_test = test_df.loc[:, train_df.columns == 'Gls']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "rmse_lin_xG = np.sqrt(-cross_val_score(lin, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error'))\n",
    "R2_lin_xG = cross_val_score(lin, X_train, y_train, cv=tscv, scoring='r2')\n",
    "\n",
    "print(f\"RMSE: {rmse_lin_xG.mean()} +/- {rmse_lin_xG.std()} - Training\")\n",
    "print(f\"R2: {R2_lin_xG.mean()} +/- {R2_lin_xG.std()} - Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19466570",
   "metadata": {},
   "source": [
    "### Score on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1443,
   "id": "07b806a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2: 0.3470078004517928 - Test\n"
     ]
    }
   ],
   "source": [
    "lin_xG=lin.fit(X_train, y_train)\n",
    "lin_xG_Test=lin_xG.score(X_test, y_test)\n",
    "print(f\"\\nR2: {lin_xG_Test} - Test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a875370",
   "metadata": {},
   "source": [
    "## 3.2. Goal Correlation with Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622a429d",
   "metadata": {},
   "source": [
    "###  Time Series Cross Validation on Training Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1444,
   "id": "9f20bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 0.23361071084689464 +/- 0.003978111207638588 - Training\n",
      "R2: 0.4910979242300389 +/- 0.025929636890849646 - Training\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.loc[:, train_df.columns != 'Gls']\n",
    "X_test = test_df.loc[:, test_df.columns != 'Gls']\n",
    "\n",
    "X_train = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
    "X_test = pd.DataFrame(preprocessor.transform(X_test))\n",
    "\n",
    "rmse_lin_model = np.sqrt(-cross_val_score(lin, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error'))\n",
    "R2_lin_model = cross_val_score(lin, X_train, y_train, cv=tscv, scoring='r2')\n",
    "\n",
    "print(f\"\\nRMSE: {rmse_lin_model.mean()} +/- {rmse_lin_model.std()} - Training\")\n",
    "print(f\"R2: {R2_lin_model.mean()} +/- {R2_lin_model.std()} - Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2536a",
   "metadata": {},
   "source": [
    "### Score on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1445,
   "id": "4a03890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R2: 0.4613344812088226 - Test\n"
     ]
    }
   ],
   "source": [
    "lin_model=lin_model.fit(X_train, y_train)\n",
    "lin_model_Test=lin_model.score(X_test, y_test)\n",
    "print(f\"\\nR2: {lin_model_Test} - Test\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f489e6",
   "metadata": {},
   "source": [
    "## 3.3. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee106036",
   "metadata": {},
   "source": [
    "### Training Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1389,
   "id": "020a7ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xG Prediction Scores:\n",
      "RMSE: 0.2595489369583568 +/- 0.007691932737236629\n",
      "R2: 0.3727887512807027 +/- 0.01631460160980845\n",
      "\n",
      "Model Prediction Scores:\n",
      "RMSE: 0.23301401532671442 +/- 0.004093134248596038\n",
      "R2: 0.49369896271862374 +/- 0.025975128826630754\n"
     ]
    }
   ],
   "source": [
    "print(\"xG Scores:\")\n",
    "print(f\"RMSE: {rmse_lin_xG.mean()} +/- {rmse_lin_xG.std()}\")\n",
    "print(f\"R2: {R2_lin_xG.mean()} +/- {R2_lin_xG.std()}\")\n",
    "\n",
    "print(\"\\nModel Scores:\")\n",
    "print(f\"RMSE: {rmse_lin_model.mean()} +/- {rmse_lin_model.std()}\")\n",
    "print(f\"R2: {R2_lin_model.mean()} +/- {R2_lin_model.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4791e7",
   "metadata": {},
   "source": [
    "Lower RMSE and higher R2 values for model vs xG. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce3fb7",
   "metadata": {},
   "source": [
    "### Test Data Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "id": "ff548bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xG Score:\n",
      "R2: 0.35162915399111705\n",
      "\n",
      "Model Score:\n",
      "R2: 0.4613344812088226\n"
     ]
    }
   ],
   "source": [
    "print(\"xG Score:\")\n",
    "print(f\"R2: {lin_xG_Test}\") \n",
    "\n",
    "print(\"\\nModel Score:\")\n",
    "print(f\"R2: {lin_model_Test}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed695cd0",
   "metadata": {},
   "source": [
    "Our results from the training data carry over onto the test data. Our model has a higher correlation with goals than xG alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3ef3d",
   "metadata": {},
   "source": [
    "# 4.0. XGB Regression Hyperparameter Optimization \n",
    "## 4.1. Before Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1422,
   "id": "4a82f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data: \n",
      "R2: 0.40177871349744954\n"
     ]
    }
   ],
   "source": [
    "lin_basic = xgb.XGBRegressor()\n",
    "lin_basic_fit=lin_basic.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Data: \")\n",
    "print(f\"R2: {lin_basic_fit.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf986e20",
   "metadata": {},
   "source": [
    "## 4.2. Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0202f46",
   "metadata": {},
   "source": [
    "XGB hyperparameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1427,
   "id": "f8b4208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate' : [.05,.1,.15, .2, .25, .3],\n",
    "    'max_depth' : range(3, 10, 1),\n",
    "    'min_child_weight' : [1, 3, 5],\n",
    "    'gamma' : [0, 1, 2, 3],\n",
    "    'colsample_bytree' : [0.5, 0.7, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400823d",
   "metadata": {},
   "source": [
    "Find optimal parameters via RandomSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042da8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "lin_params=RandomizedSearchCV(lin_basic, param_grid=params, n_iter=10, scoring='r2', cv=tscv, verbose=True)\n",
    "lin_opt_fit=lin_params.fit(X_train, y_train) \n",
    "lin_opt_model=lin_opt_fit.best_estimator_ #model with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f50e4c",
   "metadata": {},
   "source": [
    "Best parameters found by Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1438,
   "id": "a6a136c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 1,\n",
       " 'max_depth': 4,\n",
       " 'learning_rate': 0.15,\n",
       " 'gamma': 3,\n",
       " 'colsample_bytree': 0.5}"
      ]
     },
     "execution_count": 1438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_opt_fit.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2e4fe",
   "metadata": {},
   "source": [
    "## 4.3. After Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1451,
   "id": "41d0d115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default XGB Regressor on Test Data: \n",
      "R2: 0.40177871349744954\n",
      "\n",
      "Optimized XGB Regressor on Test Data: \n",
      "R2: 0.4531630601803899\n"
     ]
    }
   ],
   "source": [
    "lin_opt_model_fit=lin_opt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Default XGB Regressor on Test Data: \")\n",
    "print(f\"R2: {lin_basic_fit.score(X_test, y_test)}\")\n",
    "\n",
    "print(\"\\nOptimized XGB Regressor on Test Data: \")\n",
    "print(f\"R2: {lin_opt_model_fit.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388afcb1",
   "metadata": {},
   "source": [
    "Notable improvement after hyperparameter optimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
